"use client";
import React, { useCallback, useEffect, useState } from "react";
import SpeechRecognition, {
  useSpeechRecognition,
} from "react-speech-recognition";
import { backend_api, openvidu_api } from "@/utils/api";
import Typography from "@mui/material/Typography";
import { Grid } from "@mui/material";

interface Props {
  active: boolean;
  sessionId: string | null | undefined;
  partnerVoice: string | null | undefined;
  username: string;
  languageTarget: string;
}

const SpeechRecog = ({
  active,
  sessionId,
  partnerVoice,
  username,
  languageTarget,
}: Props) => {
  const {
    transcript,
    listening,
    resetTranscript,
    browserSupportsSpeechRecognition,
  } = useSpeechRecognition();

  const [translatedText, setTranslatedText] = useState("");

  useEffect(() => {
    if (transcript && username !== "") {
      const sendSignal = async () => {
        try {
          await openvidu_api.post(`/openvidu/api/signal`, {
            session: sessionId,
            type: "signal:voice_detected",
            data: JSON.stringify({
              username: username,
              translatedText: transcript,
            }),
          });
        } catch (err) {
          console.error("Error in signal:voice_detected:", err);
        }
      };
      sendSignal();
    }
  }, [transcript, username, sessionId]);

  useEffect(() => {
    if (active) {
      SpeechRecognition.startListening({
        continuous: true,
      });
    } else {
      SpeechRecognition.stopListening();
    }
  }, [active]);

  const fetchData = async (partnerVoice: string) => {
    console.log("ğŸ¸ğŸ¸ğŸ¸ğŸ¸ğŸ¸ğŸ¸ğŸ¸ğŸ¸", partnerVoice);
    try {
      const res = await backend_api()
        .post(`/translate?target=${languageTarget}`, {
          text: partnerVoice,
        })
        .then((res) => {
          console.log("ğŸ²ğŸ²ğŸ²ğŸ²ğŸ²ğŸ²ğŸ²ğŸ²", res.data.translatedText);
          const translatedText = res.data.translatedText || ""; // Use an empty string if translatedText is undefined
          setTranslatedText(translatedText); // ìˆì–´ì•¼ ë˜ë‚˜?
        });
    } catch (error) {
      console.error("Error fetching translation:", error);
    }
  };

  useEffect(() => {
    console.log("ğŸ§©ğŸ§©ğŸ§©ğŸ§©ğŸ§©ğŸ§©ğŸ§©", partnerVoice); // ì¶œë ¥ í™•ì¸
    if (partnerVoice) {
      fetchData(partnerVoice);
    }
  }, [partnerVoice]);

  // useEffect(() => {
  //   signalVoicesDetected();
  // }, [transcript, languageTarget]);

  useEffect(() => {
    const intervalId = setInterval(() => {
      // ì¼ì • ì‹œê°„(ì˜ˆ: 5000ms, 5ì´ˆ)ì´ ì§€ë‚œ í›„ì— ìë™ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨
      setTranslatedText("");
      resetTranscript();
    }, 5000); // 5ì´ˆë§ˆë‹¤ ìƒˆë¡œê³ ì¹¨

    return () => clearInterval(intervalId); // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œì— íƒ€ì´ë¨¸ ì •ë¦¬
  }, []);

  if (!browserSupportsSpeechRecognition) {
    return <span>ë¸Œë¼ìš°ì €ê°€ ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</span>;
  }

  return (
    <>
      <Grid item xs={12}>
        {translatedText !== null && (
          <Typography
            variant="subtitle1"
            color="secondary"
            style={{
              backgroundColor: "rgba(0, 0, 0, 0.1)",
              color: "#000000",
              padding: "8px",
              borderRadius: "4px",
              marginTop: "5px",
            }}
          >
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸: {translatedText}
          </Typography>
        )}
      </Grid>
    </>
  );
};

export default SpeechRecog;
